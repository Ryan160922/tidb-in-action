## **部署架构**
TiDB 两地三中心架构基于 Raft 算法，保证集群数据一致性和高可用。两地是同城、异地，同城双中心指在同城或临近城市建立独立数据中心，双中心通过高速链路实时同步数据，网络延迟相对较小，另外一个数据中心在异地城市。在这种场景下，可以通过控制 Reject leader 和 PD 优先级，让业务流量只派发到同城两个数据中心。

## **架构图**
以北京和西安为例阐述 TiDB 两地三中心架构部署模型，这里采用北京两个机房 IDC1 和 IDC2 ，异地西安一个机房 IDC3。北京与西安之间延迟低于3ms，北京与西安之间使用 ISP 专线，延迟约 20ms。

如下图 1 所示，为集群部署架构图，具体如下：

* 部署采用主从架构，主集群作为生产集群，承担日常生产服务，从集群通过 binlog 异步复制主集群数据库，作为备份数据库使用。
* 生产集群采用两地三中心，分别为北京 IDC1，北京 IDC2，西安 IDC3；
* 生产集群采用5副本模式，其中 IDC 和 IDC2 分别放 2 个副本，IDC3 放 1 个副本；TiKV 按机柜打 Label，既每个机柜上有一份副本。
* 从集群与主集群直接通过 binlog 完成中间数据存储与传输工作。

![图片](https://uploader.shimo.im/f/MJcAAF9gQRoEf7iV.png!thumbnail “图 1  两地三中心集群架构图 )


该架构具备高可用和容灾备份能力。相比于三数据中心方案优势如下：

* 写入速度更优。
* 两中心同时提供服务资源利用率更高。
* 可保证任一数据中心失效后，服务可用并且不发生数据丢失。

缺点很明显，因 TiDB 两地三中心基于 Raft 算法，同城两个数据中心同时失效，只有一个节点存在，不满足 Raft 算法大多数节点存在要求，最终将导致集群不可用及部分数据丢失，而且这种情况发生概率高于异地三数据中心损失概率；另外该架构成本较高。 

## 部署说明
下面具体介绍两地三中心架构部署详情。

![图片](https://uploader.shimo.im/f/MJcAAF9gQRoEf7iV.png!thumbnail "图 1  两地三中心集群架构图")

北京、西安两地三中心配置详解：

* 如图 2 所示，北京有两个机房 IDC1 和 IDC2，机房 IDC1 中有三套机架 RAC1、RAC2、RAC3，机房 IDC2 有机架 RAC4、RAC5；西安机房 IDC3 有机架 RAC6、RAC7，其中机架 RAC7 上有从集群用于备份的服务器。
* 如图中 RAC1 机架所示，TiDB、PD 服务部署在同一台服务器上，还有两台 TiKV 服务器；每台 TiKV 服务器部署 2 个 TiKV 实例，即 TiKV 服务器上每块 PCIe SSD 上单独部署一个 TiKV 实例；RAC2、RAC4、RAC5、RAC6 类似。
* 机架 RAC3 上安放 TiDB-Server 及中控+监控服务器。部署 TiDB-Server，用于日常管理维护、备份使用。中控+监控服务器上部署 TiDB-Ansible、Prometheus，Grafana 以及恢复工具；
* 从集群配置较高，采用混合部署方式，每台服务器上部署 2 个 TiKV 实例，其中的 3 台部署 TiDB 及 PD。 
* 备份服务器上部署 Mydumper 及 Dranier 以 PB 模式输出为增量备份文件。
